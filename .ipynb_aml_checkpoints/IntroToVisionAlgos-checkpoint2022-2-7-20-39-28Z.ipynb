{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Goal\n",
        "This notebook contains a few algorithms in the course intro to computer vision and their implementations in python\n",
        "## list of algorithms\n",
        "1. \n",
        "## list of libraries and frameworks needed\n",
        "the libraries used\n",
        "- numpy\n",
        "- pytorch\n",
        "- matplotlib\n",
        "- and some more...\n",
        "- TBD"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shannons sampling theorem\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## the sampling theorem\n",
        "The sampling theorem state that  \n",
        "\n",
        "---\n",
        "let $f(x)$ be a continous function. We will assume that the 'most high frequency of $f(x)$ is $\\omega$'  \n",
        "\n",
        "Then $f(x)$ can be recovered from each sample in frequency higher than $2\\omega$\n",
        "\n",
        "\n",
        "The `most high frequency` of $f(x)$ came from the fourier series of f. \n",
        "\n",
        "Let $F(u) = \\int_{x=-\\infty}^{\\infty} {f(x)e^{-2\\pi ux}dx}$ be the fourier transform of $f(x)$\n",
        "\n",
        "By the properties of fourier transform and fourier series we know, that each function $f(x)$ can be described as a sum of cosin and sins with some frequencies\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The following link may help\n",
        "https://www.youtube.com/watch?v=FcXZ28BX-xE\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### question from exam 2013\n",
        "\n",
        "1.1\tLET F(X) BE A FUNCTION. HOW CAN IT BE RESAMPLE?\n",
        "\n",
        "For simplicity, we will assume we can apply on f(x) the continuous fourier transform. If we can not, all the reconstruction process is \"invalid. We know that f(x) is composed from some sin() and cos() signals  with different frequencies (Fourier series).  \n",
        "The highest (u) with $ F(u)\\ne 0 $ when F is the fourier transform of f, is the hihest frequency appears in the fourier series of f.  \n",
        "If F is with `finite support` (i.e $\\exists_{B} \\forall_{|u|>B}(F(u)=0)$) it can be recovered in the lowest sampling frequency of 2B.\n",
        "\n",
        "This is a lower bound in some cases, and in some cases the function can be recovered in even lower frequency. however, If the sampling frequency is `uniform`, the Shannon's bound is mandatory\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import cmath\n",
        "\n",
        "def DFT2D(image):\n",
        "    data = np.asarray(image)\n",
        "    M, N = image.size # (img x, img y)\n",
        "    dft2d = np.zeros((M,N),dtype=complex)\n",
        "    for k in range(M):\n",
        "        for l in range(N):\n",
        "            sum_matrix = 0.0\n",
        "            for m in range(M):\n",
        "                for n in range(N):\n",
        "                    e = cmath.exp(- 2j * np.pi * ((k * m) / M + (l * n) / N))\n",
        "                    sum_matrix +=  data[m,n,1] * e\n",
        "            dft2d[k,l] = sum_matrix\n",
        "    return dft2d\n",
        "# this code is not relly efficient since running in for loops and not with torch operations\n",
        "img = PIL.Image.open(\"/home/azureuser/cloudfiles/code/Users/209056712/Intro_To_CV_finals/new_bnw.jpg\")\n",
        "plt.imshow(img)\n",
        "img2 = img.resize((50,50))\n",
        "plt.imshow(img2)\n",
        "dft = DFT2D(img2)\n",
        "plt.imshow(dft.real)\n",
        "# pass image through a low pass filter"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply low pass filter on an image\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply gaussian blur to an image centered in the center of the image"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use PIL for that\n",
        "from PIL import Image,ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "image = Image.open(r'/home/azureuser/cloudfiles/code/Users/209056712/IntroToCvFinals/some_image.png')\n",
        "\n",
        "image_filter = image.filter(ImageFilter.GaussianBlur)\n",
        "plt.imshow(image)\n",
        "plt.imshow(image_filter)\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "<matplotlib.image.AxesImage at 0x7ffb9ab57520>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1646407131632
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.fft\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "img = Image.open(r'new_bnw.jpg')\n",
        "img = img.convert(mode='L')\n",
        "img = np.array(img)\n",
        "img = torch.from_numpy(img)\n",
        "print(img.shape)  # (512, 512)\n",
        "\n",
        "fft_img = torch.fft.fft(img)\n",
        "print(fft_img)  # torch.Size([512, 512])\n",
        "# convert torch to an image\n",
        "torchvision.transforms.functional.to_pil_image(pic=fft_img,mode='L')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "torch.Size([2832, 4256])\ntensor([[153396.0000+0.0000j, -25890.1953-3355.9980j,\n           5932.5947-10229.9375j,  ...,\n         -19528.1523-9555.6582j,   5932.5947+10229.9375j,\n         -25890.1953+3355.9980j],\n        [153994.0000+0.0000j, -25437.0117-4117.2910j,\n           5530.1616-10598.5117j,  ...,\n         -19414.6016-9155.8867j,   5530.1616+10598.5117j,\n         -25437.0117+4117.2910j],\n        [152802.0000+0.0000j, -25206.6328-4314.8037j,\n           4979.6367-9898.8037j,  ...,\n         -19285.7266-9880.0195j,   4979.6367+9898.8037j,\n         -25206.6328+4314.8037j],\n        ...,\n        [ 36369.0000+0.0000j, -13652.7617-1162.3975j,\n           8174.8359+1343.2800j,  ...,\n           4718.9814+1418.7037j,   8174.8359-1343.2800j,\n         -13652.7617+1162.3975j],\n        [ 34409.0000+0.0000j, -12182.8145-1496.2539j,\n           6997.1436+1092.4060j,  ...,\n           5341.0122+1102.9846j,   6997.1436-1092.4060j,\n         -12182.8145+1496.2539j],\n        [ 33543.0000+0.0000j, -11013.3037-1534.9482j,\n           6074.3838+850.0256j,  ...,\n           6133.7139+662.3799j,   6074.3838-850.0256j,\n         -11013.3037+1534.9482j]])\n"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Incorrect mode (L) supplied for input type <class 'numpy.dtype'>. Should be None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7888397546dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfft_img\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# torch.Size([512, 512])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# convert torch to an image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfft_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             raise ValueError(\"Incorrect mode ({}) supplied for input type {}. Should be {}\"\n\u001b[0;32m--> 268\u001b[0;31m                              .format(mode, np.dtype, expected_mode))\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpected_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Incorrect mode (L) supplied for input type <class 'numpy.dtype'>. Should be None"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1646333123842
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations\n",
        "## Apply Affine trnasformation on an image\n",
        "Recall affine transformation is an ISOMETRIC - preserve parallelism, space and angles. by default, affine contains only 2 parts - rotation and translation, and it described by the following matrix\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "B = \n",
        "\\begin{bmatrix}\n",
        "cos(\\alpha) & -sin(\\alpha) & t_x \\\\\n",
        "sin(\\alpha) & cos(\\alpha) & t_y \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "When \n",
        "- $t_x$ is the translation in the x direction and $t_y$ the translation in the y direction\n",
        "- $\\alpha$ is the rotation matrix.\n",
        "- the new Z coordinate of a point $(x,y,z)$ is defined by the multiplication\n",
        "$$\n",
        "\\begin{equation*}\n",
        "v = \n",
        "\\begin{bmatrix}\n",
        "cos(\\alpha) & -sin(\\alpha) & t_x \\\\\n",
        "sin(\\alpha) & cos(\\alpha) & t_y \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "           x \\\\\n",
        "           y \\\\\n",
        "           z\n",
        "         \\end{bmatrix}\n",
        "= \n",
        "\\begin{bmatrix}\n",
        "\\vdots \\\\\n",
        "1*z\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "so the Z value is kept by this multiplication. Be default, the translation will be changed if the original Z value is not 1 (it will be scaled propely)\n",
        "\n",
        "Hence, `affine` transformation is not the same for each depth `Z` (as we would expect). Also, from pure math properties, we can say that the translation is scaled up in that way:\n",
        "$$ x_{new} = x_{old} +t_x*z_{old} $$\n",
        "so the translation - the distance point P is moving in the x direction is *larger* when the point is farther from the camera. This concept will come back when we will calculate optical flow.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### code implementation \n",
        "The default mode of PIL transformation is given by Image.\n",
        "Additional data would not be submitted. The only data we will have to give is the translation we want and the angle to do the transformation\n",
        "\n",
        "```python\n",
        "with Image.open(r'filepath') as image_object:\n",
        "    alpha = math.pi / 15.\n",
        "    image_object.transform((\n",
        "        math.cos(alpha), math.sin(alpha), 20,\n",
        "        -math.sin(alpha), math.cos(alpha), 20,\n",
        "        ))\n",
        "```  "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "image_object = Image.open(r'new_baby.png')\n",
        "# IMAGE_SIZE=100\n",
        "# alpha = math.pi / 15.\n",
        "# image_affine = image_object.transform(data=(\n",
        "#     math.cos(alpha), math.sin(alpha), 20,\n",
        "#     -math.sin(alpha), math.cos(alpha), 20,\n",
        "#     ),\n",
        "#     method=Image.AFFINE,\n",
        "#     size=(IMAGE_SIZE, IMAGE_SIZE)\n",
        "#     )\n",
        "image_object = img.imread(r'new_baby.png')\n",
        "plt.imshow(image_object)\n",
        "print(\"Im here\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Im here\n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1646407477160
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Apply Homography on an image\n",
        "One of the important course concepts.\n",
        "we will try to give illustration about the process\n",
        "\n",
        "Homography is detected by 8 points\n",
        "- 4 points, $\\wr(x_i,y_i)\\wr$  $i=1...4$ from the origin\n",
        "- 4 points in the destination $\\wr(a_i,b_i)\\wr$  $i=1...4$ - the function H will be have to satisfy\n",
        "\n",
        "\n",
        "$$ H((x_i,y_i)) = (a_i,b_i) $$\n",
        "\n",
        "The Homography has 8 degree of freedom, and it can be determined by a 3 by 3 Matrix with 1 in the bottom right\n",
        "corner  \n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "B = \n",
        "\\begin{bmatrix}\n",
        "a & b & c \\\\\n",
        "d & e & f \\\\\n",
        "g & h & 1\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "A way to implement is py applying image transfrom  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "# when img is PIL image\n",
        "from PIL import image\n",
        "transformed_img = img.transform(\n",
        "        size=(new_width, new_height),\n",
        "        method=Image.PERSPECTIVE,\n",
        "        data=homography_param,\n",
        "        resample=Image.BICUBIC\n",
        "    )\n",
        "```\n",
        "\n",
        "condider the following image PIL feature\n",
        "\n",
        "```python\n",
        "class QuadTransform(Transform):\n",
        "    \"\"\"\n",
        "    Define a quad image transform.\n",
        "\n",
        "    Maps a quadrilateral (a region defined by four corners) from the image to a\n",
        "    rectangle of the given size.\n",
        "\n",
        "    See :py:meth:`~PIL.Image.Image.transform`\n",
        "\n",
        "    :param xy: An 8-tuple (x0, y0, x1, y1, x2, y2, x3, y3) which contain the\n",
        "        upper left, lower left, lower right, and upper right corner of the\n",
        "        source quadrilateral.\n",
        "    \"\"\"\n",
        "\n",
        "    method = Image.QUAD\n",
        "```\n",
        "\n",
        "\n",
        "this is a rectangle\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# simple image transform\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "image = Image.open(mode='r',fp=r\"C:\\Users\\user\\Desktop\\some_image.png\")\n",
        "image.transform(\n",
        "    method=Image.QUAD,\n",
        "    size=(100,100)\n",
        ")\n",
        "# QUAD is 4 points to 4 points transformation. the defalut is to insert the new 'crop' to init at (0,0)\n",
        "# returning a new image of a given size (`PIL.Image`)\n",
        "# again we will present with `pyplotlib`\n",
        "plt.imshow(image)\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "missing method data",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m\\idan-tankel\\IntroToCvFinals\\IntroToVisionAlgos.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell://github/idan-tankel/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000011vscode-vfs?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell://github/idan-tankel/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000011vscode-vfs?line=3'>4</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m,fp\u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDesktop\u001b[39m\u001b[39m\\\u001b[39m\u001b[39msome_image.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell://github/idan-tankel/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000011vscode-vfs?line=4'>5</a>\u001b[0m image\u001b[39m.\u001b[39;49mtransform(\n\u001b[0;32m      <a href='vscode-notebook-cell://github/idan-tankel/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000011vscode-vfs?line=5'>6</a>\u001b[0m     method\u001b[39m=\u001b[39;49mImage\u001b[39m.\u001b[39;49mQUAD,\n\u001b[0;32m      <a href='vscode-notebook-cell://github/idan-tankel/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000011vscode-vfs?line=6'>7</a>\u001b[0m     size\u001b[39m=\u001b[39;49m(\u001b[39m100\u001b[39;49m,\u001b[39m100\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell://github/idan-tankel/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000011vscode-vfs?line=7'>8</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell://github/idan-tankel/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000011vscode-vfs?line=8'>9</a>\u001b[0m \u001b[39m# QUAD is 4 points to 4 points transformation. the defalut is to insert the new 'crop' to init at (0,0)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell://github/idan-tankel/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000011vscode-vfs?line=9'>10</a>\u001b[0m \u001b[39m# returning a new image of a given size (`PIL.Image`)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell://github/idan-tankel/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000011vscode-vfs?line=10'>11</a>\u001b[0m \u001b[39m# again we will present with `pyplotlib`\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell://github/idan-tankel/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000011vscode-vfs?line=11'>12</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(image)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:2451\u001b[0m, in \u001b[0;36mImage.transform\u001b[1;34m(self, size, method, data, resample, fill, fillcolor)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/PIL/Image.py?line=2447'>2448</a>\u001b[0m     method, data \u001b[39m=\u001b[39m method\u001b[39m.\u001b[39mgetdata()\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/PIL/Image.py?line=2449'>2450</a>\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/PIL/Image.py?line=2450'>2451</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmissing method data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/PIL/Image.py?line=2452'>2453</a>\u001b[0m im \u001b[39m=\u001b[39m new(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode, size, fillcolor)\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/PIL/Image.py?line=2453'>2454</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpalette:\n",
            "\u001b[1;31mValueError\u001b[0m: missing method data"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "59b4c7809edb1ed3c1f43d11707fb4ed55010b12d03d62a5a69653dee4c22b9c"
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}