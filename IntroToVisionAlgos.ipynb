{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Goal\n",
        "This notebook contains a few algorithms in the course intro to computer vision and their implementations in python\n",
        "## list of algorithms\n",
        "1. \n",
        "## list of libraries and frameworks needed\n",
        "the libraries used\n",
        "- numpy\n",
        "- pytorch\n",
        "- matplotlib\n",
        "- and some more...\n",
        "- TBD"
      ],
      "metadata": {
        "id": "0vCPyR9K5lxN",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fourier Transform\n",
        "\n",
        "The 2D fourier transforms is given in wolfram alpha\n",
        "- [https://www.wolframalpha.com/input?i=FourierTransform%5Bsin%28x%29%2Bcos%28y%29%2C%7Bx%2Cy%7D%2C%7Bu%2Cv%7D%5D]\n",
        "- [https://math.stackexchange.com/questions/780976/computing-2d-radially-symmetric-fourier-transforms-with-wolfram-alpha]\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shannons sampling theorem\n"
      ],
      "metadata": {
        "id": "VW_tzC0s5lxT",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The sampling theorem\n",
        "The sampling theorem state that  \n",
        "\n",
        "---\n",
        "let $f(x)$ be a continous function. We will assume that the 'most high frequency of $f(x)$ is $\\omega$'  \n",
        "\n",
        "Then $f(x)$ can be recovered from each sample in frequency higher than $2\\omega$\n",
        "\n",
        "\n",
        "The `most high frequency` of $f(x)$ came from the fourier series of f. \n",
        "\n",
        "Let $F(u) = \\int_{x=-\\infty}^{\\infty} {f(x)e^{-2\\pi ux}dx}$ be the fourier transform of $f(x)$\n",
        "\n",
        "By the properties of fourier transform and fourier series we know, that each function $f(x)$ can be described as a sum of cosin and sins with some frequencies\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The following link may help\n",
        "https://www.youtube.com/watch?v=FcXZ28BX-xE\n",
        "\n"
      ],
      "metadata": {
        "id": "nrb5GsQU5lxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### question from exam 2013\n",
        "\n",
        "1.1\tLET F(X) BE A FUNCTION. HOW CAN IT BE RESAMPLE?\n",
        "\n",
        "For simplicity, we will assume we can apply on f(x) the continuous fourier transform. If we can not, all the reconstruction process is \"invalid. We know that f(x) is composed from some sin() and cos() signals  with different frequencies (Fourier series).  \n",
        "The highest (u) with $ F(u)\\ne 0 $ when F is the fourier transform of f, is the hihest frequency appears in the fourier series of f.  \n",
        "If F is with `finite support` (i.e $\\exists_{B} \\forall_{|u|>B}(F(u)=0)$) it can be recovered in the lowest sampling frequency of 2B.\n",
        "\n",
        "This is a lower bound in some cases, and in some cases the function can be recovered in even lower frequency. however, If the sampling frequency is `uniform`, the Shannon's bound is mandatory\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "srSKzMqB5lxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import cmath\n",
        "\n",
        "def DFT2D(image):\n",
        "    data = np.asarray(image)\n",
        "    M, N = image.size # (img x, img y)\n",
        "    dft2d = np.zeros((M,N),dtype=complex)\n",
        "    for k in range(M):\n",
        "        for l in range(N):\n",
        "            sum_matrix = 0.0\n",
        "            for m in range(M):\n",
        "                for n in range(N):\n",
        "                    e = cmath.exp(- 2j * np.pi * ((k * m) / M + (l * n) / N))\n",
        "                    sum_matrix +=  data[m,n,1] * e\n",
        "            dft2d[k,l] = sum_matrix\n",
        "    return dft2d\n",
        "# this code is not relly efficient since running in for loops and not with torch operations\n",
        "img = PIL.Image.open(\"/home/azureuser/cloudfiles/code/Users/209056712/IntroToCVfinals/new_bnw.jpg\")\n",
        "# plt.imshow(img)\n",
        "img2 = img.resize((50,50))\n",
        "# plt.imshow(img2)\n",
        "dft = DFT2D(img2)\n",
        "plt.imshow(dft.real)\n",
        "# pass image through a low pass filter"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "8rxLoD-_5lxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply low pass filter on an image\n"
      ],
      "metadata": {
        "id": "o8_rHdLM5lxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply gaussian blur to an image centered in the center of the image"
      ],
      "metadata": {
        "id": "fxVjGjdP5lxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use PIL for that\n",
        "from PIL import Image,ImageFilter\n",
        "from matplotlib import pyplot as plt\n",
        "image = Image.open(r'/home/azureuser/cloudfiles/code/Users/209056712/IntroToCvFinals/some_image.png')\n",
        "image_filter = image.filter(ImageFilter.GaussianBlur)\n",
        "plt.imshow(image)\n",
        "plt.imshow(image_filter)\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "<matplotlib.image.AxesImage at 0x7f61cc504c70>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1646333095275
        },
        "id": "F8N_-OMS5lxZ",
        "outputId": "b562c3a5-b180-4639-f2cc-954d406e7fc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.fft\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "img = Image.open(r'new_bnw.jpg')\n",
        "img = img.convert(mode='L')\n",
        "img = np.array(img)\n",
        "img = torch.from_numpy(img)\n",
        "print(img.shape)  # (512, 512)\n",
        "\n",
        "fft_img = torch.fft.fft(img)\n",
        "print(fft_img)  # torch.Size([512, 512])\n",
        "# convert torch to an image\n",
        "torchvision.transforms.functional.to_pil_image(pic=fft_img,mode='L')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "torch.Size([2832, 4256])\ntensor([[153396.0000+0.0000j, -25890.1953-3355.9980j,\n           5932.5947-10229.9375j,  ...,\n         -19528.1523-9555.6582j,   5932.5947+10229.9375j,\n         -25890.1953+3355.9980j],\n        [153994.0000+0.0000j, -25437.0117-4117.2910j,\n           5530.1616-10598.5117j,  ...,\n         -19414.6016-9155.8867j,   5530.1616+10598.5117j,\n         -25437.0117+4117.2910j],\n        [152802.0000+0.0000j, -25206.6328-4314.8037j,\n           4979.6367-9898.8037j,  ...,\n         -19285.7266-9880.0195j,   4979.6367+9898.8037j,\n         -25206.6328+4314.8037j],\n        ...,\n        [ 36369.0000+0.0000j, -13652.7617-1162.3975j,\n           8174.8359+1343.2800j,  ...,\n           4718.9814+1418.7037j,   8174.8359-1343.2800j,\n         -13652.7617+1162.3975j],\n        [ 34409.0000+0.0000j, -12182.8145-1496.2539j,\n           6997.1436+1092.4060j,  ...,\n           5341.0122+1102.9846j,   6997.1436-1092.4060j,\n         -12182.8145+1496.2539j],\n        [ 33543.0000+0.0000j, -11013.3037-1534.9482j,\n           6074.3838+850.0256j,  ...,\n           6133.7139+662.3799j,   6074.3838-850.0256j,\n         -11013.3037+1534.9482j]])\n"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Incorrect mode (L) supplied for input type <class 'numpy.dtype'>. Should be None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7888397546dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfft_img\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# torch.Size([512, 512])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# convert torch to an image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfft_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             raise ValueError(\"Incorrect mode ({}) supplied for input type {}. Should be {}\"\n\u001b[0;32m--> 268\u001b[0;31m                              .format(mode, np.dtype, expected_mode))\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpected_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Incorrect mode (L) supplied for input type <class 'numpy.dtype'>. Should be None"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646333123842
        },
        "id": "p8FClR0H5lxb",
        "outputId": "0837ac6f-ffa8-4bd5-b3fe-f0c4125ab559"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MAzfGPS85lxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations\n",
        "## Apply Affine trnasformation on an image\n",
        "Recall affine transformation is an ISOMETRIC - preserve parallelism, space and angles. by default, affine contains only 2 parts - rotation and translation, and it described by the following matrix\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "B = \n",
        "\\begin{bmatrix}\n",
        "cos(\\alpha) & -sin(\\alpha) & t_x \\\\\n",
        "sin(\\alpha) & cos(\\alpha) & t_y \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "When \n",
        "- $t_x$ is the translation in the x direction and $t_y$ the translation in the y direction\n",
        "- $\\alpha$ is the rotation matrix.\n",
        "- the new Z coordinate of a point $(x,y,z)$ is defined by the multiplication\n",
        "$$\n",
        "\\begin{equation*}\n",
        "v = \n",
        "\\begin{bmatrix}\n",
        "cos(\\alpha) & -sin(\\alpha) & t_x \\\\\n",
        "sin(\\alpha) & cos(\\alpha) & t_y \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "           x \\\\\n",
        "           y \\\\\n",
        "           z\n",
        "         \\end{bmatrix}\n",
        "= \n",
        "\\begin{bmatrix}\n",
        "\\vdots \\\\\n",
        "1*z\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "so the Z value is kept by this multiplication. Be default, the translation will be changed if the original Z value is not 1 (it will be scaled propely)\n",
        "\n",
        "Hence, `affine` transformation is not the same for each depth `Z` (as we would expect). Also, from pure math properties, we can say that the translation is scaled up in that way:\n",
        "$$ x_{new} = x_{old} +t_x*z_{old} $$\n",
        "so the translation - the distance point P is moving in the x direction is *larger* when the point is farther from the camera. This concept will come back when we will calculate optical flow.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K4yC97f15lxd",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### code implementation \n",
        "The default mode of PIL transformation is given by Image.\n",
        "Additional data would not be submitted. The only data we will have to give is the translation we want and the angle to do the transformation\n",
        "\n",
        "```python\n",
        "with Image.open(r'filepath') as image_object:\n",
        "    alpha = math.pi / 15.\n",
        "    image_object.transform((\n",
        "        math.cos(alpha), math.sin(alpha), 20,\n",
        "        -math.sin(alpha), math.cos(alpha), 20,\n",
        "        ))\n",
        "```  "
      ],
      "metadata": {
        "id": "vHnGMi5F5lxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "with Image.open(r'/content/sample_data/new_baby.png') as image_object:\n",
        "    alpha = math.pi / 15.\n",
        "    new_object = image_object.transform(data=(\n",
        "        math.cos(alpha), math.sin(alpha), 20,\n",
        "        -math.sin(alpha), math.cos(alpha), 20,\n",
        "        ),method=Image.AFFINE,size=(1000,1000)\n",
        "    )\n",
        "    plt.imshow(new_object)\n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sample_data/new_baby.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/azureuser/cloudfiles/code/Users/209056712/IntroToCvFinals/IntroToVisionAlgos.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B66696c65706174683d2f686f6d652f617a757265757365722f636c6f756466696c65732f636f64652f55736572732f3230393035363731322f496e74726f546f437646696e616c732f496e74726f546f566973696f6e416c676f732e6970796e622b74656c656d65747279547261636b696e6749643d62376239616232652d373062622d343031632d386639322d3264663434333263393463302b73657373696f6e49643d4e362054542b6a75707974657255726c3d68747470733a2f2f7673636f6465736572766572776f722d61312e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f2b776562736f636b657455726c54656d706c6174653d7673636f6465736572766572776f722d61312d3c57535f504f52543e2e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f77732b776562736f636b6574506f72743d302b646973706c61794e616d653d7673636f6465736572766572776f722d61312b746172676574547970653d436f6d7075746520696e7374616e6365/home/azureuser/cloudfiles/code/Users/209056712/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000012vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B66696c65706174683d2f686f6d652f617a757265757365722f636c6f756466696c65732f636f64652f55736572732f3230393035363731322f496e74726f546f437646696e616c732f496e74726f546f566973696f6e416c676f732e6970796e622b74656c656d65747279547261636b696e6749643d62376239616232652d373062622d343031632d386639322d3264663434333263393463302b73657373696f6e49643d4e362054542b6a75707974657255726c3d68747470733a2f2f7673636f6465736572766572776f722d61312e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f2b776562736f636b657455726c54656d706c6174653d7673636f6465736572766572776f722d61312d3c57535f504f52543e2e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f77732b776562736f636b6574506f72743d302b646973706c61794e616d653d7673636f6465736572766572776f722d61312b746172676574547970653d436f6d7075746520696e7374616e6365/home/azureuser/cloudfiles/code/Users/209056712/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000012vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n\u001b[0;32m----> <a href='vscode-notebook-cell://amlext%2B66696c65706174683d2f686f6d652f617a757265757365722f636c6f756466696c65732f636f64652f55736572732f3230393035363731322f496e74726f546f437646696e616c732f496e74726f546f566973696f6e416c676f732e6970796e622b74656c656d65747279547261636b696e6749643d62376239616232652d373062622d343031632d386639322d3264663434333263393463302b73657373696f6e49643d4e362054542b6a75707974657255726c3d68747470733a2f2f7673636f6465736572766572776f722d61312e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f2b776562736f636b657455726c54656d706c6174653d7673636f6465736572766572776f722d61312d3c57535f504f52543e2e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f77732b776562736f636b6574506f72743d302b646973706c61794e616d653d7673636f6465736572766572776f722d61312b746172676574547970653d436f6d7075746520696e7374616e6365/home/azureuser/cloudfiles/code/Users/209056712/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000012vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m Image\u001b[39m.\u001b[39;49mopen(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/content/sample_data/new_baby.png\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m image_object:\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B66696c65706174683d2f686f6d652f617a757265757365722f636c6f756466696c65732f636f64652f55736572732f3230393035363731322f496e74726f546f437646696e616c732f496e74726f546f566973696f6e416c676f732e6970796e622b74656c656d65747279547261636b696e6749643d62376239616232652d373062622d343031632d386639322d3264663434333263393463302b73657373696f6e49643d4e362054542b6a75707974657255726c3d68747470733a2f2f7673636f6465736572766572776f722d61312e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f2b776562736f636b657455726c54656d706c6174653d7673636f6465736572766572776f722d61312d3c57535f504f52543e2e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f77732b776562736f636b6574506f72743d302b646973706c61794e616d653d7673636f6465736572766572776f722d61312b746172676574547970653d436f6d7075746520696e7374616e6365/home/azureuser/cloudfiles/code/Users/209056712/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000012vscode-remote?line=4'>5</a>\u001b[0m     alpha \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mpi \u001b[39m/\u001b[39m \u001b[39m15.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B66696c65706174683d2f686f6d652f617a757265757365722f636c6f756466696c65732f636f64652f55736572732f3230393035363731322f496e74726f546f437646696e616c732f496e74726f546f566973696f6e416c676f732e6970796e622b74656c656d65747279547261636b696e6749643d62376239616232652d373062622d343031632d386639322d3264663434333263393463302b73657373696f6e49643d4e362054542b6a75707974657255726c3d68747470733a2f2f7673636f6465736572766572776f722d61312e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f2b776562736f636b657455726c54656d706c6174653d7673636f6465736572766572776f722d61312d3c57535f504f52543e2e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f77732b776562736f636b6574506f72743d302b646973706c61794e616d653d7673636f6465736572766572776f722d61312b746172676574547970653d436f6d7075746520696e7374616e6365/home/azureuser/cloudfiles/code/Users/209056712/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000012vscode-remote?line=5'>6</a>\u001b[0m     new_object \u001b[39m=\u001b[39m image_object\u001b[39m.\u001b[39mtransform(data\u001b[39m=\u001b[39m(\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B66696c65706174683d2f686f6d652f617a757265757365722f636c6f756466696c65732f636f64652f55736572732f3230393035363731322f496e74726f546f437646696e616c732f496e74726f546f566973696f6e416c676f732e6970796e622b74656c656d65747279547261636b696e6749643d62376239616232652d373062622d343031632d386639322d3264663434333263393463302b73657373696f6e49643d4e362054542b6a75707974657255726c3d68747470733a2f2f7673636f6465736572766572776f722d61312e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f2b776562736f636b657455726c54656d706c6174653d7673636f6465736572766572776f722d61312d3c57535f504f52543e2e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f77732b776562736f636b6574506f72743d302b646973706c61794e616d653d7673636f6465736572766572776f722d61312b746172676574547970653d436f6d7075746520696e7374616e6365/home/azureuser/cloudfiles/code/Users/209056712/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000012vscode-remote?line=6'>7</a>\u001b[0m         math\u001b[39m.\u001b[39mcos(alpha), math\u001b[39m.\u001b[39msin(alpha), \u001b[39m20\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B66696c65706174683d2f686f6d652f617a757265757365722f636c6f756466696c65732f636f64652f55736572732f3230393035363731322f496e74726f546f437646696e616c732f496e74726f546f566973696f6e416c676f732e6970796e622b74656c656d65747279547261636b696e6749643d62376239616232652d373062622d343031632d386639322d3264663434333263393463302b73657373696f6e49643d4e362054542b6a75707974657255726c3d68747470733a2f2f7673636f6465736572766572776f722d61312e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f2b776562736f636b657455726c54656d706c6174653d7673636f6465736572766572776f722d61312d3c57535f504f52543e2e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f77732b776562736f636b6574506f72743d302b646973706c61794e616d653d7673636f6465736572766572776f722d61312b746172676574547970653d436f6d7075746520696e7374616e6365/home/azureuser/cloudfiles/code/Users/209056712/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000012vscode-remote?line=7'>8</a>\u001b[0m         \u001b[39m-\u001b[39mmath\u001b[39m.\u001b[39msin(alpha), math\u001b[39m.\u001b[39mcos(alpha), \u001b[39m20\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B66696c65706174683d2f686f6d652f617a757265757365722f636c6f756466696c65732f636f64652f55736572732f3230393035363731322f496e74726f546f437646696e616c732f496e74726f546f566973696f6e416c676f732e6970796e622b74656c656d65747279547261636b696e6749643d62376239616232652d373062622d343031632d386639322d3264663434333263393463302b73657373696f6e49643d4e362054542b6a75707974657255726c3d68747470733a2f2f7673636f6465736572766572776f722d61312e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f2b776562736f636b657455726c54656d706c6174653d7673636f6465736572766572776f722d61312d3c57535f504f52543e2e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f77732b776562736f636b6574506f72743d302b646973706c61794e616d653d7673636f6465736572766572776f722d61312b746172676574547970653d436f6d7075746520696e7374616e6365/home/azureuser/cloudfiles/code/Users/209056712/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000012vscode-remote?line=8'>9</a>\u001b[0m         ),method\u001b[39m=\u001b[39mImage\u001b[39m.\u001b[39mAFFINE,size\u001b[39m=\u001b[39m(\u001b[39m1000\u001b[39m,\u001b[39m1000\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B66696c65706174683d2f686f6d652f617a757265757365722f636c6f756466696c65732f636f64652f55736572732f3230393035363731322f496e74726f546f437646696e616c732f496e74726f546f566973696f6e416c676f732e6970796e622b74656c656d65747279547261636b696e6749643d62376239616232652d373062622d343031632d386639322d3264663434333263393463302b73657373696f6e49643d4e362054542b6a75707974657255726c3d68747470733a2f2f7673636f6465736572766572776f722d61312e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f2b776562736f636b657455726c54656d706c6174653d7673636f6465736572766572776f722d61312d3c57535f504f52543e2e776573746575726f70652e696e7374616e6365732e617a7572656d6c2e6d732f77732b776562736f636b6574506f72743d302b646973706c61794e616d653d7673636f6465736572766572776f722d61312b746172676574547970653d436f6d7075746520696e7374616e6365/home/azureuser/cloudfiles/code/Users/209056712/IntroToCvFinals/IntroToVisionAlgos.ipynb#ch0000012vscode-remote?line=9'>10</a>\u001b[0m     )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/PIL/Image.py:2891\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   <a href='file:///anaconda/envs/azureml_py38/lib/python3.8/site-packages/PIL/Image.py?line=2887'>2888</a>\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   <a href='file:///anaconda/envs/azureml_py38/lib/python3.8/site-packages/PIL/Image.py?line=2889'>2890</a>\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> <a href='file:///anaconda/envs/azureml_py38/lib/python3.8/site-packages/PIL/Image.py?line=2890'>2891</a>\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   <a href='file:///anaconda/envs/azureml_py38/lib/python3.8/site-packages/PIL/Image.py?line=2891'>2892</a>\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   <a href='file:///anaconda/envs/azureml_py38/lib/python3.8/site-packages/PIL/Image.py?line=2893'>2894</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/new_baby.png'"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "executionInfo": {
          "elapsed": 600,
          "status": "ok",
          "timestamp": 1646632163085,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "N-yq-C7K5lxe",
        "outputId": "79b20b92-1bb2-4b1b-ba7d-a9ce00c9dead"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Apply Homography on an image\n",
        "One of the important course concepts.\n",
        "we will try to give illustration about the process\n",
        "\n",
        "Homography is detected by 8 points\n",
        "- 4 points, $\\wr(x_i,y_i)\\wr$  $i=1...4$ from the origin\n",
        "- 4 points in the destination $\\wr(a_i,b_i)\\wr$  $i=1...4$ - the function H will be have to satisfy\n",
        "\n",
        "\n",
        "$$ H((x_i,y_i)) = (a_i,b_i) $$\n",
        "\n",
        "The Homography has 8 degree of freedom, and it can be determined by a 3 by 3 Matrix with 1 in the bottom right\n",
        "corner  \n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "B = \n",
        "\\begin{bmatrix}\n",
        "a & b & c \\\\\n",
        "d & e & f \\\\\n",
        "g & h & 1\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "A way to implement is py applying image transfrom  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "# when img is PIL image\n",
        "from PIL import image\n",
        "transformed_img = img.transform(\n",
        "        size=(new_width, new_height),\n",
        "        method=Image.PERSPECTIVE,\n",
        "        data=homography_param,\n",
        "        resample=Image.BICUBIC\n",
        "    )\n",
        "```\n",
        "\n",
        "condider the following image PIL feature\n",
        "\n",
        "```python\n",
        "class QuadTransform(Transform):\n",
        "    \"\"\"\n",
        "    Define a quad image transform.\n",
        "\n",
        "    Maps a quadrilateral (a region defined by four corners) from the image to a\n",
        "    rectangle of the given size.\n",
        "\n",
        "    See :py:meth:`~PIL.Image.Image.transform`\n",
        "\n",
        "    :param xy: An 8-tuple (x0, y0, x1, y1, x2, y2, x3, y3) which contain the\n",
        "        upper left, lower left, lower right, and upper right corner of the\n",
        "        source quadrilateral.\n",
        "    \"\"\"\n",
        "\n",
        "    method = Image.QUAD\n",
        "```\n",
        "\n",
        "\n",
        "this is a rectangle\n"
      ],
      "metadata": {
        "id": "6HjObAT45lxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simple image transform\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "image = Image.open(mode='r',fp=r'./new_baby.png')\n",
        "new_image = image.transform(\n",
        "    method=Image.QUAD,\n",
        "    data=(0,0,1000,0,1000,1000,0,1000),\n",
        "    size=(1000,1000)\n",
        ")\n",
        "# QUAD is 4 points to 4 points transformation. the defalut is to insert the new 'crop' to init at (0,0)\n",
        "# returning a new image of a given size (`PIL.Image`)\n",
        "# again we will present with `pyplotlib`\n",
        "plt.imshow(new_image)\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "<matplotlib.image.AxesImage at 0x7f61cbe50250>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "executionInfo": {
          "elapsed": 862,
          "status": "ok",
          "timestamp": 1646632713386,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "eAXUo71A5lxg",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "0f2e40e8-178c-43a8-810f-4ca375afa629"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "as you see, homography can be relly unclear from the first site. Things that are not kept\n",
        "\n",
        "\n",
        "*   Distance (between points)\n",
        "*   *Parallel lines*\n",
        "And all the image structure might be changed\n",
        "\n",
        "The **Data** argument is a 4 points ordered within a tuple, they are the *edges* of a quadradic shape (מרובע) taken from the image\n",
        "\n",
        "### How to recover homogrhapy that was done on 2 images?\n",
        "\n",
        "The question we want to tackle - let's say we have got image A, of a baby. We have passwd it to unknown homography, and got image B. Can we, given image A and image B, recover the homography parameters?\n",
        "\n",
        "Since Homography is vector marix multiplication, linear algebra would state we would need 8 equations for that (since we have got 8 params)\n",
        "#### important!\n",
        "in order to recover, we would need also *correspondence!* since we need to solve equations from the sort\n",
        "$$\n",
        "\\begin{equation*}\n",
        "\\begin{bmatrix}\n",
        "u \\\\\n",
        "v \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "= \n",
        "\\begin{bmatrix}\n",
        "a & b & c \\\\\n",
        "d & e & f \\\\\n",
        "g & h & 1\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x \\\\\n",
        "y \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "we need to find (or to know) for (x,y) the matched (u,v) in the other frame.\n",
        "In general, this would lead us **to 9x9 matrix!**\n",
        "\n",
        "\n",
        "\n",
        "A proper function for that exists in the *OPENCV library* under this link\n",
        "\n",
        "A general algorithm for that is given by the **SVD** decomposition of the matrix. See also [https://math.stackexchange.com/questions/3509039/calculate-homography-with-and-without-svd]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v4aOZ4PJ8Lt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PVe3fxAk8oZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_homography(image1,image2)\n",
        "\"\"\"\n",
        "This function is used to find homography between image1 and image2\n",
        "If there is no possible homography between the images, this shoud return an error\n",
        "\"\"\"\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of IntroToVisionAlgos.ipynb",
      "provenance": [
        {
          "file_id": "https://github.com/idan-tankel/IntroToCvFinals/blob/main/IntroToVisionAlgos.ipynb",
          "timestamp": 1646632915968
        }
      ],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "59b4c7809edb1ed3c1f43d11707fb4ed55010b12d03d62a5a69653dee4c22b9c"
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}